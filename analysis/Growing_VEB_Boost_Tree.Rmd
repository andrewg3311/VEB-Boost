---
title: "Growing a VEB Boost Tree"
author: "Andrew Goldstein"
date: "January 15, 2020"
output:
  workflowr::wflow_html:
    code_folding: show
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, error = F, warning = F, message = F, fig.align = 'center', autodep = T)
```


# Introduction
So far, we have assumed that we have a given structure for the VEB-Boost tree, defined by the tree structure $T(\mu_1, \dots, \mu_L)$. However, part of the appeal of the VEB-Boost tree is that the structure of the tree can be learned adaptively from the data! This page outlines a few methods for growing the VEB-Boost tree.


# Pre-Defining a Large VEB-Boost Tree
The most obvious place to start is to ignore the fact that we can grow the VEB-Boost tree, and just start with a "large" predefined tree structure, optimize the ELBO, and call it a day. For instance, we could start with:
$$
T(\mu_1, \dots, \mu_{K \cdot d}) = \sum_{k=1}^K \prod_{j=1}^d \mu_{d \cdot (k-1) + j}
$$
In other words, our tree is the sum of $K$ learners, each being the product of $d$ base learners. This mirrors traditional boosting, if we consider the product of base learners to be similar to a small regression tree. This also aligns with [Boosting Products of Base Classifiers](https://dl.acm.org/doi/10.1145/1553374.1553439).

We could also modify this so that each learner in the sum has its own "depth", e.g. some of them are stumps ($d = 1$), some of them are the product of two terks ($d = 2$), etc.

The main benefit of this method is that we don't have to "waste" time fitting small VEB-Boost trees, only to grow them and re-fit all over again.

The main drawbacks of this method are:

1. If the relationship in the data is relatively simple, a much smaller VEB-Boost tree would have been sufficient, and we could have found that structure much quicker; and 

2. If the relationship in the data is very complex, our big starting tree might not be sufficiently complex to describe the relationship well.

Note that because this is a Bayesian method, the risk of over-fitting is lower than in traditional boosting, so I would guess that starting with a big tree that is overly complex isn't much of a problem from that standpoint, but I have not tested this yet.


# Growing the VEB-Boost Tree Iteratively
One way to adaptively grow the VEB-Boost tree is as follows:

Given a starting VEB-Boost tree structure (already fitted to convergence), we determine a set of base learners to split up into a more complex structure. Specifically, we replace a base learner $\mu_0$ with a learner of the form $(\mu_0 \circ \mu_2) + \mu_1$. We keep the fitted distribution at $\mu_0$ to be the same as it was, and initialize $\mu_1$ and $\mu_2$ to be constant functions, $\mu_1 \equiv 0$ and $\mu_2 \equiv 1$. This way, the overall fit remains unchanged when we incorporate these new base learners.

In order to determine which base learners we split up in this way, we can define a decision rule based on the fitted distribution. In the case of the SER, I have been using a threshold for the fitted value of the prior variance, $\sigma_0^2$. E.g. if the fitted prior variance is $\sigma_0^2 \lt .001$, we say that the base learner is effectively constant. From then on, we then consider that base learner to be constant, and change the way that base learner fits the data, so that we fit a constant function rather than, e.g. a SER. We consider these base learners to be "locked". For internal nodes in the VEB-Boost tree, if all descendent base learners are constant, then we consider that internal node to be locked as well.

Once we determing which base learners are locked, we then look at the non-constant base learners. If a base learner's sibling learner is locked, and it's parent's sibling is locked, then there is no need to split up that base learner. As a result, we consider this base learner to be locked as well.

Once we have determined which base learners are not locked, we perform the splitting procedure as outlined above. We can then fit this new VEB-Boost tree to our desired level of convergence, and repeat the procedure until either

1. All base learners are locked; or

2. The change in the ELBO from the old VEB-Boost tree structure to the new one is small.

The benefits and drawbacks of this method are the reverse of the above method.

## A Note on Convergence
Since we are going to change the VEB-Boost tree structure after convergence, it appears that it would be to our benefit if we have a loose convergence criterion to start and progressively tighted it as our VEB-Boost tree grows. However, empirically this doesn't usually appear to be the case. As far as I can tell, the reason is because our fit of the smaller tree is like an initialization for our fit of the larger tree, so the better our initialization, the better our solution. And since each pass over a smaller tree with our CAVI algorithm is faster than a pass over a larger VEB-Boost tree, we'd rather do as much convergence on a smaller tree as we can.
