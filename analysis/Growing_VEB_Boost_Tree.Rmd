---
title: "Growing a VEB Boost Tree"
author: "Andrew Goldstein"
date: "September 19, 2019"
output:
  workflowr::wflow_html:
    code_folding: show
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, error = F, warning = F, message = F, fig.align = 'center', autodep = T)
```


# Introduction





# Examples
Below, I load the VEB-Boost-Node object, and define some functions needed for running a weighted SuSiE SER.
```{r}
library(data.tree)
source("./code/VEB_Boost_Node.R") # load VEB_Boost_Node object

# function for calculating 2nd moment for weighted SER
calc_Q = function(Sigma2, Mu, Alpha) {
  # Sigma2[j, l] is the posterior variance for b_l when entry j is selected, p x L
  # Mu[j, l] is the posterior mean for b_l when entry j is selected, p x L
  # Alpha[j, l] is the posterior probability selecting entry j from b_l, p x L
  # Z is matrix of covariates (e.g. column for intercept, top 10 PCs, etc)
  # delta is current estimate for effects of Z variables
  
  ASU2 = Alpha * (Sigma2 + Mu^2) # [j, l] = alpha[j, l] * (Sigma2[j, l] + Mu[j, l]^2)
  
  Q = rowSums(ASU2)
  
  return(Q)
  
}

# function for calculating KL divergence for weighted SER
calc_KL = function(Mu, Alpha, Sigma2, prior_var = 1) {
  p = nrow(Mu)
  L = ncol(Mu)
  prior_weights = rep(1/p, p)
  P = matrix(prior_weights, nrow = p, ncol = L)
  b_post = rowSums(Alpha * Mu)
  
  prior_var = matrix(prior_var, nrow = p, ncol = L, byrow = T)
  
  KL_div = Alpha * (log(Alpha) - log(P) + (log(prior_var) / 2) - (log(Sigma2) / 2) - .5 + ((Sigma2 + Mu^2) / (2 * prior_var)))
  KL_div[Alpha == 0] = 0
  return(sum(KL_div))
}
```

```{r}
log_lik_SER = function(V, tau_no_V, nu, sigma2, prior_weights) {
  if (length(sigma2) == 1) {
    sigma2 = rep(sigma2, length(nu))
  }
  tau = tau_no_V + (1 / V)
  m = -(log(tau) / 2) + (nu^2 / (2 * tau))
  m_max = max(m)
  w = exp(m - m_max)
  -(log(V) / 2) + m_max + log(sum(prior_weights * w))
}

neg.loglik.logscale = function(lV, tau_no_V, nu, sigma2, prior_weights){
  -log_lik_SER(exp(lV), tau_no_V, nu, sigma2, prior_weights)
}

optimize_V = function(tau_no_V, nu, sigma2, prior_weights, V = 1) {
  lV = optim(par = log(V), fn = neg.loglik.logscale, tau_no_V = tau_no_V, nu = nu, sigma2 = sigma2, prior_weights = prior_weights, method='Brent', lower = -10, upper = 15)$par
  V = exp(lV)
  return(V)
}
```

```{r}
# function to perform weighted SER
weighted_SER = function(X, Y, sigma2, init = list(V = NULL)) {
  if (length(sigma2) == 1) {
    sigma2 = rep(sigma2, nrow(X))
  }
  
  X_avg = apply(X, MARGIN = 2, function(col) weighted.mean(col, 1/sigma2))
  Y_avg = weighted.mean(Y, 1/sigma2)
  X_avg_mat = matrix(X_avg, nrow = nrow(X), ncol = ncol(X), byrow = T)
  X_cent = X - X_avg_mat
  Y_cent = Y - Y_avg
  
  prior_weights = rep(1 / ncol(X_cent), ncol(X_cent))
  tau_no_V = (t(X_cent^2) %*% (1 / sigma2))
  nu = colSums(X_cent * Y_cent / sigma2)
  
  V = ifelse(is.null(init$V), 1, init$V)
  
  V = optimize_V(tau_no_V, nu, sigma2, prior_weights, V)

  tau = tau_no_V + (1 / V)
  
  alpha = log(prior_weights) - (.5 * log(tau)) + (.5 * nu^2 / tau)
  alpha = alpha - max(alpha)
  alpha = exp(alpha)
  alpha = alpha / sum(alpha)
  
  mu = nu / tau
  
  sigma2_post = 1 / tau
  
  beta_post_1 = rowSums(alpha * mu)
  beta_post_2 = calc_Q(sigma2_post, mu, alpha)
  intercept = as.numeric(Y_avg - X_avg%*%beta_post_1)
  
  mu1 = X %*% beta_post_1 + intercept
  mu2 = intercept^2 + 2*intercept*(X %*% beta_post_1) + (X^2 %*% beta_post_2)
  
  if (any(mu2 < mu1^2)) { # sanity check, this should be >= 0
    stop("Predicted variance is negative")
  }
  
  KL_div = calc_KL(mu, alpha, sigma2_post, V)
  
  return(list(mu1 = as.numeric(mu1), mu2 = as.numeric(mu2), KL_div = KL_div, alpha = alpha, mu = mu, sigma2_post = sigma2_post, intercept = intercept, V = V, X_avg = X_avg, Y_avg = Y_avg))
}

predFn = function(X_new, currentFit, moment = c(1, 2)) {
  beta_post_1 = currentFit$alpha * currentFit$mu
  if (moment == 1) {
    return(as.numeric(X_new %*% beta_post_1 + currentFit$intercept))
  } else if (moment == 2) {
    beta_post_2 = calc_Q(currentFit$sigma2_post, currentFit$mu, currentFit$alpha)
    return(as.numeric(currentFit$intercept^2 + 2*currentFit$intercept*(X_new %*% beta_post_1) + (X_new^2 %*% beta_post_2)))
  } else {
    stop("`moment` must be either 1 or 2")
  }
}
```


```{r}
weighted_SER_no_int = function(X, Y, sigma2, init = list(intercept = NULL, V = NULL)) {
  if (length(sigma2) == 1) {
    sigma2 = rep(sigma2, nrow(X))
  }
  prior_weights = rep(1 / ncol(X), ncol(X))
  
  X_avg = apply(X, MARGIN = 2, function(col) weighted.mean(col, 1/sigma2))
  Y_avg = weighted.mean(Y, 1/sigma2)
  
  intercept = ifelse(is.null(init$intercept), Y_avg, init$intercept)
  Y = Y - intercept
  
  tau_no_V = (t(X^2) %*% (1 / sigma2))
  nu = colSums(X * Y / sigma2)

  V = ifelse(is.null(init$V), 1, init$V)
  
  V = optimize_V(tau_no_V, nu, sigma2, prior_weights, V)

  tau = tau_no_V + (1 / V)
  
  alpha = log(prior_weights) - (.5 * log(tau)) + (.5 * nu^2 / tau)
  alpha = alpha - max(alpha)
  alpha = exp(alpha)
  alpha = alpha / sum(alpha)
  
  mu = nu / tau
  
  sigma2_post = 1 / tau
  
  beta_post_1 = rowSums(alpha * mu)
  beta_post_2 = calc_Q(sigma2_post, mu, alpha)
  intercept = as.numeric(Y_avg - (X_avg %*% beta_post_1))
  
  predFunction = function(X_new, moment = c(1, 2)) {
    if (moment == 1) {
      return(as.numeric(X_new %*% (beta_post_1) + intercept))
    } else if (moment == 2) {
      return(as.numeric(intercept^2 + 2*intercept*(X_new %*% beta_post_1) + ((X_new^2) %*% beta_post_2)))
    } else {
      stop("`moment` must be either 1 or 2")
    }
  }
  # 
  mu1 = predFunction(X, 1)
  mu2 = predFunction(X, 2)
    
  # if (any(mu2 < mu1^2)) { # sanity check, this should be >= 0
  #   stop("Predicted variance is negative")
  # }
  
  KL_div = calc_KL(mu, alpha, sigma2_post, V)
  
  return(list(mu1 = as.numeric(mu1), mu2 = as.numeric(mu2), KL_div = KL_div, alpha = alpha, mu = mu, sigma2_post = sigma2_post, intercept = intercept, V = V, predFunction = predFunction))
}


fitFn_no_int = function(X, Y, sigma2, init) {
  res = weighted_SER_no_int(X, Y, sigma2, init)
  return(res)
}



weighted_SER_no_int2 = function(X, Y, sigma2, init = list(intercept = NULL, V = NULL)) {
  if (length(sigma2) == 1) {
    sigma2 = rep(sigma2, nrow(X))
  }
  prior_weights = rep(1 / ncol(X), ncol(X))
  
  X_avg = apply(X, MARGIN = 2, function(col) weighted.mean(col, 1/sigma2))
  Y_avg = weighted.mean(Y, 1/sigma2)
  
  intercept = ifelse(is.null(init$intercept), Y_avg, init$intercept)
  Y = Y - intercept
  
  tau_no_V = (t(X^2) %*% (1 / sigma2))
  nu = colSums(X * Y / sigma2)

  V = ifelse(is.null(init$V), 1, init$V)
  
  V = optimize_V(tau_no_V, nu, sigma2, prior_weights, V)

  tau = tau_no_V + (1 / V)
  
  alpha = log(prior_weights) - (.5 * log(tau)) + (.5 * nu^2 / tau)
  alpha = alpha - max(alpha)
  alpha = exp(alpha)
  alpha = alpha / sum(alpha)
  
  mu = nu / tau
  
  sigma2_post = 1 / tau
  
  beta_post_1 = rowSums(alpha * mu)
  beta_post_2 = calc_Q(sigma2_post, mu, alpha)
  intercept = as.numeric(Y_avg - (X_avg %*% beta_post_1))
  
  predFunction = function(X_new, currentFit, moment = c(1, 2)) {
    beta_post_1 = rowSums(currentFit$alpha * currentFit$mu)
    if (moment == 1) {
      return(as.numeric(X_new %*% (beta_post_1) + currentFit$intercept))
    } else if (moment == 2) {
        ASU2 = currentFit$alpha * (currentFit$sigma2 + currentFit$mu^2) # [j, l] = alpha[j, l] * (Sigma2[j, l] + Mu[j, l]^2)
        beta_post_2 = rowSums(ASU2)
        return(as.numeric(currentFit$intercept^2 + 2*currentFit$intercept*(X_new %*% beta_post_1) + ((X_new^2) %*% beta_post_2)))
    } else {
      stop("`moment` must be either 1 or 2")
    }
  }

    
  mu1 = as.numeric(X %*% (beta_post_1) + intercept)
  mu2 = as.numeric(intercept^2 + 2*intercept*(X %*% beta_post_1) + ((X^2) %*% beta_post_2))
  
  # if (any(mu2 < mu1^2)) { # sanity check, this should be >= 0
  #   stop("Predicted variance is negative")
  # }
  
  KL_div = calc_KL(mu, alpha, sigma2_post, V)
  
  return(list(mu1 = as.numeric(mu1), mu2 = as.numeric(mu2), KL_div = KL_div, alpha = alpha, mu = mu, sigma2_post = sigma2_post, intercept = intercept, V = V, predFunction = predFunction))
}


fitFn_no_int2 = function(X, Y, sigma2, init) {
  res = weighted_SER_no_int2(X, Y, sigma2, init)
  return(res)
}
```


```{r}
set.seed(1138)

mean_Y = function(X) {
  10*sin(pi * X[, 1] * X[, 2]) + 20*(X[, 3] - .5)^2 + 10*X[, 4] + 5*X[, 5]
}

n = 1000
p = 10

X = matrix(runif(n*p), nrow = n, ncol = p)
mu_true = mean_Y(X)
Y = mu_true + rnorm(n)

n_new = 1000
X_new = matrix(runif(n_new*p), nrow = n_new, ncol = p)
mu_true_new = mean_Y(X_new)

system.time({
  mu_02 = VEBBoostNode$new("mu_0", fitFunction = fitFn_no_int, currentFit = list(mu1 = 0, mu2 = 1, KL_div = 1))
  mu_02$Y = Y
  mu_02$sigma2 = 1
  mu_02$convergeFit(update_ELBO_progress = T, update_sigma2 = F)
  
  cat(mean((mu_02$mu1 - mu_true)^2))
  cat("\n")
  
  i = 1
  learner_name = paste("mu_", i, sep = '')
  combine_name = paste("combine_", i, sep = '')
  learner2 = mu_02$addLearner(learner_name, combine_name, fitFn_no_int, tol = 1e-3)
  learner2$convergeFit(1e-3, update_ELBO_progress = T, update_sigma2 = F)
  
  cat(mean((learner2$mu1 - mu_true)^2))
  cat("\n")
  
  for (i in 2:20) {
    learner_name = paste("mu_", i, sep = '')
    combine_name = paste("combine_", i, sep = '')
    learner2$addLearner(learner_name, combine_name, fitFn_no_int, tol = 1e-3)
    learner2$convergeFit(1e-3, update_ELBO_progress = T, update_sigma2 = F)
    cat(mean((learner2$mu1 - mu_true)^2))
    cat("\n")
  }
})


learner2$predict.veb(X_new)
sqrt(mean((learner2$pred_mu1 - mu_true_new)^2))
```



```{r}
weighted_SER = function(X, Y, sigma2, init = list(V = NULL)) {
  if (length(sigma2) == 1) {
    sigma2 = rep(sigma2, nrow(X))
  }
  
  X_avg = apply(X, MARGIN = 2, function(col) weighted.mean(col, 1/sigma2))
  Y_avg = weighted.mean(Y, 1/sigma2)
  X_avg_mat = matrix(X_avg, nrow = nrow(X), ncol = ncol(X), byrow = T)
  X_cent = X - X_avg_mat
  Y_cent = Y - Y_avg
  
  prior_weights = rep(1 / ncol(X_cent), ncol(X_cent))
  
  V = ifelse(is.null(init$V), 1, init$V)
  
  tau_no_V = (t(X_cent^2) %*% (1 / sigma2))
  nu = colSums(X_cent * Y / sigma2)

  V = optimize_V(tau_no_V, nu, sigma2, prior_weights, V)

  tau = tau_no_V + (1 / V)
  
  alpha = log(prior_weights) - (.5 * log(tau)) + (.5 * nu^2 / tau)
  alpha = alpha - max(alpha)
  alpha = exp(alpha)
  alpha = alpha / sum(alpha)
  
  mu = nu / tau
  
  sigma2_post = 1 / tau
  
  beta_post_1 = rowSums(alpha * mu)
  beta_post_2 = calc_Q(sigma2_post, mu, alpha)
  intercept = as.numeric(Y_avg - X_avg%*%beta_post_1)
  
  predFunction = function(X_new, moment = c(1, 2)) {
    if (moment == 1) {
      return(X_new %*% (beta_post_1) + intercept)
    } else if (moment == 2) {
        X_avg_mat_new = matrix(X_avg, nrow = nrow(X_new), ncol = ncol(X_new), byrow = T)
        X_cent_new = X_new - X_avg_mat_new
        return(as.numeric(as.numeric(sum(1 / sigma2)^(-1) + (Y_avg^2) + 2*Y_avg*(X_cent_new %*% beta_post_1) - 2*((X_new * X_avg_mat_new) %*% beta_post_2) + (X_new^2 + X_avg_mat_new^2) %*% beta_post_2)))
    } else {
      stop("`moment` must be either 1 or 2")
    }
  }
  
  mu1 = predFunction(X, 1)
  mu2 = predFunction(X, 2)
  
  if (any(mu2 < mu1^2)) { # sanity check, this should be >= 0
    stop("Predicted variance is negative")
  }
  
  KL_div = calc_KL(mu, alpha, sigma2_post, V)
  
  return(list(mu1 = as.numeric(mu1), mu2 = as.numeric(mu2), KL_div = KL_div, alpha = alpha, mu = mu, sigma2_post = sigma2_post, intercept = intercept, V = V, predFunction = predFunction))
}

fitFn = function(X, Y, sigma2, init) {
  res = weighted_SER(X, Y, sigma2, init)
  return(res)
}
```


```{r}
# does not account for variability of intercept
# OR DOES IT? intercept is a function of beta (which is variable)
predFn2 = function(X_new, currentFit, moment = c(1, 2)) {
  X_avg_mat_new = matrix(currentFit$X_avg, nrow = nrow(X_new), ncol = ncol(X_new), byrow = T)
  X_cent_new = X_new - X_avg_mat_new
  beta_post_1 = rowSums(currentFit$alpha * currentFit$mu)
  if (moment == 1) {
    return(as.numeric(currentFit$Y_avg + (X_cent_new %*% (beta_post_1))))
  } else if (moment == 2) {
      ASU2 = currentFit$alpha * (currentFit$sigma2 + currentFit$mu^2) # [j, l] = alpha[j, l] * (Sigma2[j, l] + Mu[j, l]^2)
      beta_post_2 = rowSums(ASU2)
      return(as.numeric((currentFit$Y_avg^2) + 2*currentFit$Y_avg*(X_cent_new %*% beta_post_1) + ((X_cent_new^2) %*% beta_post_2)))
  } else {
    stop("`moment` must be either 1 or 2")
  }
}

weighted_SER2 = function(X, Y, sigma2, init = list(V = NULL)) {
  if (length(sigma2) == 1) {
    sigma2 = rep(sigma2, nrow(X))
  }
  
  X_avg = apply(X, MARGIN = 2, function(col) weighted.mean(col, 1/sigma2))
  Y_avg = weighted.mean(Y, 1/sigma2)
  X_avg_mat = matrix(X_avg, nrow = nrow(X), ncol = ncol(X), byrow = T)
  X_cent = X - X_avg_mat
  Y_cent = Y - Y_avg
  
  prior_weights = rep(1 / ncol(X_cent), ncol(X_cent))
  
  V = ifelse(is.null(init$V), 1, init$V)
  
  tau_no_V = (t(X_cent^2) %*% (1 / sigma2))
  #nu = colSums(X_cent * Y / sigma2)
  nu = colSums(X_cent * Y_cent / sigma2)
  # same as: t(X_cent) %*% (Y_cent / sigma2)
  
  V = optimize_V(tau_no_V, nu, sigma2, prior_weights, V)

  tau = tau_no_V + (1 / V)
  
  alpha = log(prior_weights) - (.5 * log(tau)) + (.5 * nu^2 / tau)
  alpha = alpha - max(alpha)
  alpha = exp(alpha)
  alpha = alpha / sum(alpha)
  
  mu = nu / tau
  
  sigma2_post = 1 / tau
  
  beta_post_1 = rowSums(alpha * mu)
  beta_post_2 = calc_Q(sigma2_post, mu, alpha)
  intercept = as.numeric(Y_avg - X_avg%*%beta_post_1)
  
  mu1 = predFn2(X, list(alpha = alpha, mu = mu, sigma2_post = sigma2_post, intercept = intercept, X_avg = X_avg, Y_avg = Y_avg), 1)
  mu2 = predFn2(X, list(alpha = alpha, mu = mu, sigma2_post = sigma2_post, intercept = intercept, X_avg = X_avg, Y_avg = Y_avg), 2)
  
  # if (any(mu2 < mu1^2)) { # sanity check, this should be >= 0
  #   browser()
  #   stop("Predicted variance is negative")
  # }
  
  KL_div = calc_KL(mu, alpha, sigma2_post, V)
  
  return(list(mu1 = as.numeric(mu1), mu2 = as.numeric(mu2), KL_div = KL_div, alpha = alpha, mu = mu, sigma2_post = sigma2_post, intercept = intercept, V = V, X_avg = X_avg, Y_avg = Y_avg))
}

# independent normal prior on intercept
weighted_SER2_int = function(X, Y, sigma2, init = list(V = NULL, intercept = NULL)) {
  if (length(sigma2) == 1) {
    sigma2 = rep(sigma2, nrow(X))
  }
  
  sum_inv_sigma2 = sum(1 / sigma2)
  
  intercept = ifelse(is.null(init$intercept), 0, init$intercept)
  
  Y_cent = Y - intercept
  
  prior_weights = rep(1 / ncol(X), ncol(X))
  
  V = ifelse(is.null(init$V), 1, init$V)
  
  tau_no_V = (t(X^2) %*% (1 / sigma2))
  nu = colSums(X * Y_cent / sigma2)
  
  V = optimize_V(tau_no_V, nu, sigma2, prior_weights, V)

  tau = tau_no_V + (1 / V)
  
  alpha = log(prior_weights) - (.5 * log(tau)) + (.5 * nu^2 / tau)
  alpha = alpha - max(alpha)
  alpha = exp(alpha)
  alpha = alpha / sum(alpha)
  
  mu = nu / tau
  
  sigma2_post = 1 / tau
  
  beta_post_1 = rowSums(alpha * mu)
  beta_post_2 = calc_Q(sigma2_post, mu, alpha)
  
  mu1 = X %*% beta_post_1
  mu2 = X^2 %*% beta_post_2
  
    # EB for intercept
  Y_int = Y - mu1
  nu_int = sum(Y_int / sigma2)
  V_int = max(0, ((nu_int^2 / sum_inv_sigma2) - 1) / sum_inv_sigma2)
  tau_int = (1 / V_int) + sum_inv_sigma2
  intercept = ifelse(V_int == 0, 0, nu_int / tau_int)
  mu2_int = ifelse(V_int == 0, 0, 1 / tau_int + intercept^2)
  
  mu2 = mu2 + 2*mu1*intercept + mu2_int
  mu1 = mu1 + intercept

  # if (any(mu2 < mu1^2)) { # sanity check, this should be >= 0
  #   browser()
  #   stop("Predicted variance is negative")
  # }
  
  KL_div_int = ifelse(V_int == 0, 0, ((log(V_int) + log(tau_int)) + (((1 / tau_int) + (intercept^2)) /  V_int) - 1) / 2) # KL div from Q_int to prior
  
  KL_div = calc_KL(mu, alpha, sigma2_post, V) + KL_div_int
  
  return(list(mu1 = as.numeric(mu1), mu2 = as.numeric(mu2), KL_div = KL_div, alpha = alpha, mu = mu, sigma2_post = sigma2_post, intercept = intercept, V = V, V_int = V_int))
}
```

```{r}
# does not have an intercept at all
weighted_SER3 = function(X, Y, sigma2, init = list(V = NULL)) {
  if (length(sigma2) == 1) {
    sigma2 = rep(sigma2, nrow(X))
  }
  
  prior_weights = rep(1 / ncol(X), ncol(X))
  
  V = ifelse(is.null(init$V), 1, init$V)
  
  tau_no_V = (t(X_cent^2) %*% (1 / sigma2))
  nu = colSums(X_cent * Y / sigma2)

  V = optimize_V(tau_no_V, nu, sigma2, prior_weights, V)

  tau = tau_no_V + (1 / V)
  
  alpha = log(prior_weights) - (.5 * log(tau)) + (.5 * nu^2 / tau)
  alpha = alpha - max(alpha)
  alpha = exp(alpha)
  alpha = alpha / sum(alpha)
  
  mu = nu / tau
  
  sigma2_post = 1 / tau
  
  beta_post_1 = rowSums(alpha * mu)
  beta_post_2 = calc_Q(sigma2_post, mu, alpha)
  
  predFunction = function(X_new, moment = c(1, 2)) {
    if (moment == 1) {
      return(as.numeric(X_new %*% beta_post_1))
    } else if (moment == 2) {
      return(as.numeric((X_new^2) %*% beta_post_2))
    } else {
      stop("`moment` must be either 1 or 2")
    }
  }

  mu1 = predFunction(X, 1)
  mu2 = predFunction(X, 2)
  
  if (any(mu2 < mu1^2)) { # sanity check, this should be >= 0
    stop("Predicted variance is negative")
  }
  
  KL_div = calc_KL(mu, alpha, sigma2_post, V)
  
  return(list(mu1 = as.numeric(mu1), mu2 = as.numeric(mu2), KL_div = KL_div, alpha = alpha, mu = mu, sigma2_post = sigma2_post, V = V, predFunction = predFunction))
}
```

```{r}
system.time({
  mu_03 = VEBBoostNode$new("mu_0", fitFunction = fitFn, currentFit = list(mu1 = 0, mu2 = 1, KL_div = 1))
  mu_03$Y = Y
  mu_03$sigma2 = 1
  mu_03$convergeFit(update_ELBO_progress = T, update_sigma2 = F)
  
  cat(mean((mu_03$mu1 - mu_true)^2))
  cat("\n")
  
  i = 1
  learner_name = paste("mu_", i, sep = '')
  combine_name = paste("combine_", i, sep = '')
  learner3 = mu_03$addLearner(learner_name, combine_name, fitFn, tol = 1e-3)
  learner3$convergeFit(1e-3, update_ELBO_progress = T, update_sigma2 = F)
  
  cat(mean((learner3$mu1 - mu_true)^2))
  cat("\n")
  
  for (i in 2:20) {
    learner_name = paste("mu_", i, sep = '')
    combine_name = paste("combine_", i, sep = '')
    learner3$addLearner(learner_name, combine_name, fitFn, tol = 1e-3)
    learner3$convergeFit(1e-3, update_ELBO_progress = T, update_sigma2 = F)
    cat(mean((learner3$mu1 - mu_true)^2))
    cat("\n")
  }
})


learner3$predict.veb(X_new)
sqrt(mean((learner3$pred_mu1 - mu_true_new)^2))
```

```{r}
system.time({
  mu_04 = VEBBoostNode$new("mu_0", fitFunction = fitFn, currentFit = list(mu1 = 0, mu2 = 1, KL_div = 1))
  mu_04$Y = Y
  mu_04$sigma2 = 1
  mu_04$convergeFit(update_ELBO_progress = T, update_sigma2 = F)
  
  cat(mean((mu_04$mu1 - mu_true)^2))
  cat("\n")
  
  learner4 = mu_04$convergeFitEB(1e-3, update_ELBO_progress = T, update_sigma2 = F)
  cat(mean((learner4$mu1 - mu_true)^2))
  cat("\n")
  
  while (length(Traverse(learner4, filterFun = function(node) node$isLeaf & (node$ebCombineOperator != "."))) > 0) {
    learner4$convergeFitEB(1e-3, update_ELBO_progress = T, update_sigma2 = F)
    cat(mean((learner4$mu1 - mu_true)^2))
    cat("\n")
  }
})


learner4$predict.veb(X_new)
sqrt(mean((learner4$pred_mu1 - mu_true_new)^2))
```

```{r}
# calc_KL_full = function(mean_list, precision_list, alpha, sigma2, V) {
#   KL_div = 0
#   for (j in 1:length(mean_list)) {
#     det_prec = det(precision_list[[j]])
#     KL_div = KL_div + alpha[j]*.5*(
#       (sum(1 / sigma2) / (det_prec * V)) + (mean_list[[j]][2] / V) - 2 + log(V) + log(det_prec)
#     )
#   }
#   return(KL_div)
# }
# 
# # full Bayesian prior on intercept
# weighted_SER_full = function(X, Y, sigma2, init = list(V = NULL)) {
#   n = nrow(X)
#   p = ncol(X)
#   
#   if (length(sigma2) == 1) {
#     sigma2 = rep(sigma2, n)
#   }
#   
#   prior_weights = rep(1/p, p)
#   
#   # EB for V
#   X_avg = apply(X, MARGIN = 2, function(col) weighted.mean(col, 1/sigma2))
#   Y_avg = weighted.mean(Y, 1/sigma2)
#   X_avg_mat = matrix(X_avg, nrow = nrow(X), ncol = ncol(X), byrow = T)
#   X_cent = X - X_avg_mat
#   Y_cent = Y - Y_avg
#   
#   V = ifelse(is.null(init$V), 1, init$V)
#   
#   V = optimize_V(X_cent, Y_cent, sigma2, prior_weights, V)
#   
#   mean_list = vector("list", p)
#   precision_list = vector("list", p)
#   
#   for (j in 1:p) {
#     precision_list[[j]] = matrix(c(sum(1 / sigma2), sum(X[, j] / sigma2), sum(X[, j] / sigma2), 1/V + sum(X[, j]^2 / sigma2)), nrow = 2, ncol = 2, byrow = T)
#     mean_list[[j]] = as.numeric(solve(precision_list[[j]], c(sum(Y / sigma2), sum(X[, j] * Y / sigma2))))
#   }
#   
#   alpha = log(prior_weights) - (.5 * sapply(precision_list, function(x) log(det(x)))) + (.5 * sapply(1:p, function(j) sum(mean_list[[j]] * c(sum(Y / sigma2), sum(Y*X[, j] / sigma2)))))
#   alpha = alpha - max(alpha)
#   alpha = exp(alpha)
#   alpha = alpha / sum(alpha)
#   
#   post_1 = alpha * do.call(rbind, mean_list)
#   int_post_1 = sum(post_1[, 1])
#   b_post_1 = post_1[, 2]
#   
#   predFunction = function(X_new, moment = c(1, 2)) {
#     if (moment == 1) {
#       mu1 = int_post_1 + (X_new %*% b_post_1)
#       return(as.numeric(mu1))
#     } else if (moment == 2) {
#       mu2 = numeric(nrow(X_new))
#       for (j in 1:ncol(X_new)) {
#         mat_j = cbind(1, X_new[, j])
#         mu2 = mu2 + alpha[j]*(rowSums(mat_j * (mat_j %*% solve(precision_list[[j]]))) + (mat_j %*% mean_list[[j]])^2)
#       }
#       return(as.numeric(mu2))
#     } else {
#       stop("`moment` must be either 1 or 2")
#     }
#   }
#   
#   mu1 = predFunction(X, 1)
#   mu2 = predFunction(X, 2)
#   
#   if (any(mu2 < mu1^2)) { # sanity check, this should be >= 0
#     stop("Predicted variance is negative")
#   }
#   
#   KL_div = calc_KL_full(mean_list, precision_list, alpha, sigma2, V)
#   
#   return(list(mu1 = as.numeric(mu1), mu2 = as.numeric(mu2), KL_div = KL_div, alpha = alpha, 
#               mean_list = mean_list, precision_list = precision_list, V = V, predFunction = predFunction))
# }
# 
# fitFnFull = function(X, Y, sigma2, init) {
#   res = weighted_SER_full(X, Y, sigma2, init)
#   return(res)
# }

calc_KL_full = function(mean_list, precision_list, alpha, sigma2, V) {
  KL_div = 0
  for (j in 1:length(mean_list)) {
    det_prec = det(precision_list[[j]])
    KL_div = KL_div + alpha[j]*.5*(
      (sum(1 / sigma2) / (det_prec * V)) + (mean_list[[j]][2]^2 / V) - 2 + log(V) + log(det_prec)
    )
  }
  return(KL_div)
}

predFnFull = function(X_new, currentFit, moment = c(1, 2)) {
  post_1 = currentFit$alpha * do.call(rbind, currentFit$mean_list)
  int_post_1 = sum(post_1[, 1])
  b_post_1 = post_1[, 2]
  if (moment == 1) {
    mu1 = int_post_1 + (X_new %*% b_post_1)
    return(as.numeric(mu1))
  } else if (moment == 2) {
    mu2 = numeric(nrow(X_new))
    for (j in 1:ncol(X_new)) {
      mat_j = cbind(1, X_new[, j])
      mu2 = mu2 + currentFit$alpha[j]*(rowSums(mat_j * (mat_j %*% solve(currentFit$precision_list[[j]]))) + (mat_j %*% currentFit$mean_list[[j]])^2)
    }
    return(as.numeric(mu2))
  } else {
    stop("`moment` must be either 1 or 2")
  }
}

# full Bayesian prior on intercept
weighted_SER_full = function(X, Y, sigma2, init = list(V = NULL)) {
  n = nrow(X)
  p = ncol(X)

  if (length(sigma2) == 1) {
    sigma2 = rep(sigma2, n)
  }

  prior_weights = rep(1/p, p)

  # EB for V
  X_avg = apply(X, MARGIN = 2, function(col) weighted.mean(col, 1/sigma2))
  Y_avg = weighted.mean(Y, 1/sigma2)
  X_avg_mat = matrix(X_avg, nrow = nrow(X), ncol = ncol(X), byrow = T)
  X_cent = X - X_avg_mat
  Y_cent = Y - Y_avg

  V = ifelse(is.null(init$V), 1, init$V)

  tau_no_V = (t(X_cent^2) %*% (1 / sigma2))
  nu = colSums(X_cent * Y / sigma2)

  V = optimize_V(tau_no_V, nu, sigma2, prior_weights, V)

  mean_list = vector("list", p)
  precision_list = vector("list", p)

  for (j in 1:p) {
    precision_list[[j]] = matrix(c(sum(1 / sigma2), sum(X[, j] / sigma2), sum(X[, j] / sigma2), 1/V + sum(X[, j]^2 / sigma2)), nrow = 2, ncol = 2, byrow = T)
    mean_list[[j]] = as.numeric(solve(precision_list[[j]], c(sum(Y / sigma2), sum(X[, j] * Y / sigma2))))
  }

  #alpha = log(prior_weights) - (.5 * sapply(precision_list, function(x) log(det(x)))) + (.5 * sapply(1:p, function(j) sum(mean_list[[j]] * c(sum(Y / sigma2), sum(Y*X[, j] / sigma2)))))
  alpha = log(prior_weights) - (.5 * sapply(precision_list, function(x) log(det(x)))) + (.5 * sapply(1:p, function(j) t(mean_list[[j]]) %*% precision_list[[j]] %*% mean_list[[j]]))
  alpha = alpha - max(alpha)
  alpha = exp(alpha)
  alpha = alpha / sum(alpha)

  mu1 = predFnFull(X, list(alpha = alpha, mean_list = mean_list, precision_list = precision_list), 1)
  mu2 = predFnFull(X, list(alpha = alpha, mean_list = mean_list, precision_list = precision_list), 2)

  if (any(mu2 < mu1^2)) { # sanity check, this should be >= 0
    stop("Predicted variance is negative")
  }

  KL_div = calc_KL_full(mean_list, precision_list, alpha, sigma2, V)

  return(list(mu1 = as.numeric(mu1), mu2 = as.numeric(mu2), KL_div = KL_div, alpha = alpha,
              mean_list = mean_list, precision_list = precision_list, V = V))
}

fitFnFull = function(X, Y, sigma2, init) {
  res = weighted_SER_full(X, Y, sigma2, init)
  return(res)
}
```


```{r}
set.seed(1138)

# mean_Y = function(X) {
#   10*sin(pi * X[, 1] * X[, 2]) + 20*(X[, 3] - .5)^2 + 10*X[, 4] + 5*X[, 5]
# }
mean_Y = function(X) {
  5*sin(3*X[, 1]) + 2*(X[, 2]^2) + 3*X[, 3]*X[, 4]
}
# mean_Y = function(X) {
#   apply(X[, 1:3], MARGIN = 1, max)
# }
# mean_Y = function(X) {
#   (5*sin(3*X[, 1]) + 2*(X[, 2]^2) + 3*X[, 3]*X[, 4])*(abs(X[, 1]) <= .5) +
#     apply(X[, 1:3], MARGIN = 1, max)*(abs(X[, 1]) > .5)
# }

n = 100
p = 10

X = matrix(rnorm(n*p), nrow = n, ncol = p)
# X = matrix(runif(n*p), nrow = n, ncol = p)
mu_true = mean_Y(X)
Y = mu_true + rnorm(n)

X_stumps = make_stumps_matrix(X, T, apply(X, MARGIN = 2, function(col) quantile(col, probs = seq(from = 0, to = 1, length.out = 100))))

# attr(X, 'order') = apply(X, MARGIN = 2, order, decreasing = T)
# attr(X, 'rank') = apply(X, MARGIN = 2, rank)

n_new = n
X_new = matrix(rnorm(n_new*p), nrow = n_new, ncol = p)
# X_new = matrix(runif(n_new*p), nrow = n_new, ncol = p)
mu_true_new = mean_Y(X_new)

# attr(X_new, 'X') = X

X_new_stumps = make_stumps_matrix(X_new, T, do.call(cbind, sapply(X_stumps, function(x) attr(x, 'br'))))

fitFn = function(X, Y, sigma2, init) {
  res = weighted_SER(X, Y, sigma2, init)
  return(res)
}

fitFn2 = function(X, Y, sigma2, init) {
  res = weighted_SER2(X, Y, sigma2, init)
  return(res)
}

fitFn2_int = function(X, Y, sigma2, init) {
  res = weighted_SER2_int(X, Y, sigma2, init)
  return(res)
}

fitFn3 = function(X, Y, sigma2, init) {
  res = weighted_SER3(X, Y, sigma2, init)
  return(res)
}

fitFn4 = function(X, Y, sigma2, init) {
  res = weighted_SER_full(X, Y, sigma2, init)
  return(res)
}



tol = 1e-1
system.time({
  mu_05 = VEBBoostNodeComp$new("mu_0", fitFunction = fitFn2, predFunction = predFn2, currentFit = list(mu1 = 0, mu2 = 1, KL_div = 1))
  #mu_05 = VEBBoostNode$new("mu_0", fitFunction = fitFn, currentFit = list(predFunction = null_add_pred, KL_div = 1))
  mu_05$X = X
  mu_05$Y = Y
  mu_05$sigma2 = 1
  mu_05$convergeFit(update_ELBO_progress = T, update_sigma2 = F)
  
  cat(mean((mu_05$mu1 - mu_true)^2))
  cat("\n")
  
  learner5 = mu_05$convergeFitEB(tol, update_ELBO_progress = T, update_sigma2 = F)
  cat(mean((learner5$mu1 - mu_true)^2))
  cat("\n")
  
  while (length(Traverse(learner5, filterFun = function(node) node$isLeaf & (node$ebCombineOperator != "."))) > 0) {
    learner5$convergeFitEB(tol, update_ELBO_progress = T, update_sigma2 = F)
    cat(mean((learner5$mu1 - mu_true)^2))
    cat("\n")
  }
})

X_new_stumps = make_X_new_stumps(X, X_new)
learner5$predict.veb(X_new)
sqrt(mean((learner5$pred_mu1 - mu_true_new)^2))

system.time({
  bart.fit.def = BayesTree::bart(x.train = X, y.train = Y, x.test = X_new, ndpost = 3000, nskip = 1000)
  })
sqrt(mean((bart.fit.def$yhat.test.mean - mu_true_new)^2))

system.time({
  xbart.fit = XBART(Y, X, X_new, num_trees = 200, num_sweeps = 40, burnin = 15, alpha = .95, beta = 2)
})
sqrt(mean((xbart.fit$yhats_test - mu_true_new)^2))
```

```{r}
system.time({
  mu_06 = VEBBoostNodeComp$new("mu_0", fitFunction = fitFn2, predFunction = predFn2, currentFit = list(mu1 = 0, mu2 = 1, KL_div = 1))
  # mu_06 = VEBBoostNodeComp$new("mu_0", fitFunction = fitFn_no_int2, currentFit = list(mu1 = 0, mu2 = 1, KL_div = 1))
  #mu_05 = VEBBoostNode$new("mu_0", fitFunction = fitFn, currentFit = list(predFunction = null_add_pred, KL_div = 1))
  mu_06$X = X
  mu_06$Y = Y
  mu_06$sigma2 = 1
  mu_06$convergeFit(update_ELBO_progress = T, update_sigma2 = F)
  
  cat(mean((mu_06$mu1 - mu_true)^2))
  cat("\n")
  
  learner6 = mu_06$convergeFitEB(tol, update_ELBO_progress = T, update_sigma2 = F)
  cat(mean((learner6$mu1 - mu_true)^2))
  cat("\n")
  
  while (length(Traverse(learner6, filterFun = function(node) node$isLeaf & (node$ebCombineOperator != "."))) > 0) {
    learner6$convergeFitEB(tol, update_ELBO_progress = T, update_sigma2 = F, V_tol = 1e-3)
    cat(mean((learner6$mu1 - mu_true)^2))
    cat("\n")
  }
})

learner6$predict.veb(X_new)
sqrt(mean((learner6$pred_mu1 - mu_true_new)^2))
```



```{r}
set.seed(1138)

fentonFn = function(X) {
  (12 + X[, 1]^2 + ((1 + X[, 1] + X[, 2]) / X[, 1]^2) + ((100 + (X[, 1]^2 * X[, 2]^2)) / ((X[, 1] * X[, 2])^4))) / 10
}

n = 10000
p = 10

# X = matrix(rnorm(n*p), nrow = n, ncol = p)
X = matrix(runif(n*p, .75, 5), nrow = n, ncol = p)
mu_true = fentonFn(X)
Y = mu_true + rnorm(n)

attr(X, 'order') = apply(X, MARGIN = 2, order, decreasing = T)
attr(X, 'rank') = apply(X, MARGIN = 2, rank)

n_new = n
# X_new = matrix(rnorm(n_new*p), nrow = n_new, ncol = p)
X_new = matrix(runif(n_new*p, .75, 5), nrow = n_new, ncol = p)
mu_true_new = fentonFn(X_new)

attr(X_new, 'X') = X

tol = 1e-1
system.time({
  mu_07 = VEBBoostNodeComp$new("mu_0", fitFunction = fitFn2, predFunction = predFn2, currentFit = list(mu1 = 0, mu2 = 1, KL_div = 1))
  #mu_05 = VEBBoostNode$new("mu_0", fitFunction = fitFn, currentFit = list(predFunction = null_add_pred, KL_div = 1))
  mu_07$X = X
  mu_07$Y = Y
  mu_07$sigma2 = 1
  mu_07$convergeFit(update_ELBO_progress = T, update_sigma2 = F)
  
  cat(mean((mu_07$mu1 - mu_true)^2))
  cat("\n")
  
  learner7 = mu_07$convergeFitEB(tol, update_ELBO_progress = T, update_sigma2 = F)
  cat(mean((learner7$mu1 - mu_true)^2))
  cat("\n")
  
  while (length(Traverse(learner7, filterFun = function(node) node$isLeaf & (node$ebCombineOperator != "."))) > 0) {
    learner7$convergeFitEB(tol, update_ELBO_progress = T, update_sigma2 = F, V_tol = 1e-3)
    cat(mean((learner7$mu1 - mu_true)^2))
    cat("\n")
  }
})

learner7$predict.veb(X_new)
sqrt(mean((learner7$pred_mu1 - mu_true_new)^2))

system.time({
  bart.fit.def = BayesTree::bart(x.train = X, y.train = Y, x.test = X_new, ndpost = 3000, nskip = 1000)
})
sqrt(mean((bart.fit.def$yhat.test.mean - mu_true_new)^2))
#mean(Y_new != apply(bart.fit.def$yhat.test, 2, function(x) mean(x >= 0) >= .5))

#options(java.parameters = "-Xmx5000m")
# system.time({
#   bart.machine.fit.def = bartMachine::bartMachine(data.frame(X), Y, num_burn_in = 1000, num_iterations_after_burn_in = 3000, num_trees = 200)
# })

system.time({
  xbart.fit = XBART::XBART(Y, X, X_new, num_trees = 200, num_sweeps = 40, burnin = 15, alpha = .95, beta = 2, num_cutpoints = 100)
})
sqrt(mean((xbart.fit$yhats_test - mu_true_new)^2))

system.time({
  gbm.fit = gbm::gbm(Y ~ ., distribution = 'gaussian', data = data.frame(X, Y), interaction.depth = 4)
})
sqrt(mean((gbm::predict.gbm(gbm.fit, data.frame(X_new), n.trees = gbm.fit$n.trees) - mu_true_new)^2))

system.time({
  rf.fit = randomForest::randomForest(X, Y, X_new, mu_true_new)
})
sqrt(mean((rf.fit$test$predicted - mu_true_new)^2))
```


```{r}
# try updating sigma2 ONLY AFTER we converge the fit on the given tree, and only once
# still sometimes get an issue w/ estimating sigma2 way too high, and under-fitting
system.time({
  mu_08 = VEBBoostNodeComp$new("mu_0", fitFunction = fitFn2, predFunction = predFn2, currentFit = list(mu1 = 0, mu2 = 1, KL_div = 1))
  #mu_05 = VEBBoostNode$new("mu_0", fitFunction = fitFn, currentFit = list(predFunction = null_add_pred, KL_div = 1))
  mu_08$X = X
  mu_08$Y = Y
  mu_08$sigma2 = 1
  mu_08$convergeFit(update_ELBO_progress = T, update_sigma2 = F)
  mu_08$update_sigma2()
  
  cat(mean((mu_08$mu1 - mu_true)^2))
  cat("\n")
  
  learner8 = mu_08$convergeFitEB(tol, update_ELBO_progress = T, update_sigma2 = F)
  learner8$update_sigma2()
  cat(mean((learner8$mu1 - mu_true)^2))
  cat("\n")
  
  while (length(Traverse(learner8, filterFun = function(node) node$isLeaf & (node$ebCombineOperator != "."))) > 0) {
    learner8$convergeFitEB(tol, update_ELBO_progress = T, update_sigma2 = F, V_tol = 1e-3)
    learner8$update_sigma2()
    cat(mean((learner8$mu1 - mu_true)^2))
    cat("\n")
  }
})
learner8$predict.veb(X_new)
sqrt(mean((learner8$pred_mu1 - mu_true_new)^2))
```

```{r}
system.time({
  mu_09 = VEBBoostNodeComp$new("mu_0", fitFunction = fitFn2, predFunction = predFn2, currentFit = list(mu1 = 0, mu2 = 1, KL_div = 1))
  #mu_05 = VEBBoostNode$new("mu_0", fitFunction = fitFn, currentFit = list(predFunction = null_add_pred, KL_div = 1))
  mu_09$X = X
  mu_09$Y = Y
  mu_09$sigma2 = var(Y)
  mu_09$convergeFit(update_ELBO_progress = T, update_sigma2 = T)
  #mu_09$update_sigma2()
  
  cat(mean((mu_09$mu1 - mu_true)^2))
  cat("\n")
  
  learner9 = mu_09$convergeFitAll(tol, update_ELBO_progress = T, update_sigma2 = T)
  #learner9$update_sigma2()
  cat(mean((learner9$mu1 - mu_true)^2))
  cat("\n")
  
  while ((abs(tail(tail(learner9$ELBO_progress, 1)[[1]], 1) - tail(tail(learner9$ELBO_progress, 2)[[1]], 1)) > tol) && (length(Traverse(learner9, filterFun = function(node) node$isLeaf & !node$isLocked)) > 0)) {
    learner9$convergeFitAll(tol, update_ELBO_progress = T, update_sigma2 = T, V_tol = 1e-3)
    #learner9$update_sigma2()
    cat(mean((learner9$mu1 - mu_true)^2))
    cat("\n")
  }
})
learner9$predict.veb(X_new, 1)
sqrt(mean((learner9$pred_mu1 - mu_true_new)^2))
#mean(Y_new != (learner9$pred_mu1 >= 0))
#sum(Y_new*log(1 + exp(-learner9$pred_mu1)) + (1-Y_new)*log(1 + exp(learner9$pred_mu1)))
```


```{r}
system.time({
    mu_010 = VEBBoostNode$new("mu_0", fitFunction = fitFn, predFunction = predFn, currentFit = list(mu1 = 0, mu2 = 1, KL_div = 1))
    #mu_05 = VEBBoostNode$new("mu_0", fitFunction = fitFn, currentFit = list(predFunction = null_add_pred, KL_div = 1))
    mu_010$X = X_stumps
    mu_010$Y = Y
    mu_010$sigma2 = var(Y)
    mu_010$convergeFit(update_ELBO_progress = T, update_sigma2 = T)
    #mu_09$update_sigma2()
    
    cat(mean((mu_010$mu1 - mu_true)^2))
    cat("\n")
    
    learner10 = mu_010$convergeFitAll(tol, update_ELBO_progress = T, update_sigma2 = T)
    #learner9$update_sigma2()
    cat(mean((learner10$mu1 - mu_true)^2))
    cat("\n")
    
    while ((abs(tail(tail(learner10$ELBO_progress, 1)[[1]], 1) - tail(tail(learner10$ELBO_progress, 2)[[1]], 1)) > tol) && (length(Traverse(learner10, filterFun = function(node) node$isLeaf & !node$isLocked)) > 0)) {
        learner10$convergeFitAll(tol, update_ELBO_progress = T, update_sigma2 = T, V_tol = 1e-3)
        #learner9$update_sigma2()
        cat(mean((learner10$mu1 - mu_true)^2))
        cat("\n")
    }
})
learner10$predict.veb(X_new_stumps, 1)
sqrt(mean((learner10$pred_mu1 - mu_true_new)^2))
```